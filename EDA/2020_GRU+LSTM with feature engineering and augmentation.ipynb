{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### GRU+LSTM with feature engineering and augmentation\n",
    "```\n",
    "前処理でデータを増やすのと特徴量エンジニアリングをしてる。このやり方は使えそうだし参考になるなあ\n",
    "```\n",
    "### https://www.kaggle.com/its7171/gru-lstm-with-feature-engineering-and-augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### memo eda\n",
    "https://www.kaggle.com/allohvk/eda-some-additional-features-rc-pair-distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import tensorflow.keras.layers as L\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "from sklearn.cluster import KMeans\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOW TO GENERATE\n",
    "## https://www.kaggle.com/its7171/how-to-generate-augmentation-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough GPU hardware devices available\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "def allocate_gpu_memory(gpu_number=0):\n",
    "    physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "    if physical_devices:\n",
    "        try:\n",
    "            print(\"Found {} GPU(s)\".format(len(physical_devices)))\n",
    "            tf.config.set_visible_devices(physical_devices[gpu_number], 'GPU')\n",
    "            tf.config.experimental.set_memory_growth(physical_devices[gpu_number], True)\n",
    "            print(\"#{} GPU memory is allocated\".format(gpu_number))\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"Not enough GPU hardware devices available\")\n",
    "allocate_gpu_memory()\n",
    "\n",
    "Ver='GRU_LSTM1'\n",
    "# ここから取れる。\n",
    "# そのまま使っちゃっていいんじゃないかな\n",
    "# https://www.kaggle.com/its7171/how-to-generate-augmentation-data\n",
    "aug_data = '../input/openvaccine-augmentation-data/aug_data1.csv'\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru_layer(hidden_dim, dropout):\n",
    "    return L.Bidirectional(L.GRU(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))\n",
    "\n",
    "def lstm_layer(hidden_dim, dropout):\n",
    "    return L.Bidirectional(L.LSTM(hidden_dim, dropout=dropout, return_sequences=True, kernel_initializer = 'orthogonal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(seq_len=107, pred_len=68, dropout=0.5, embed_dim=100, hidden_dim=256, type=0):\n",
    "    \n",
    "    # 前処理で入力データを６次元にしてるの面白いなあ \n",
    "    inputs = L.Input(shape=(seq_len, 6))\n",
    "    \n",
    "    # split categorical and numerical features and concatenate them later.\n",
    "    # inputs からデータを選択してる\n",
    "    categorical_feat_dim = 3\n",
    "    categorical_fea = inputs[:, :, :categorical_feat_dim]\n",
    "    numerical_fea = inputs[:, :, 3:]\n",
    "\n",
    "    # embed layer\n",
    "    embed = L.Embedding(input_dim=len(token2int), output_dim=embed_dim)(categorical_fea)\n",
    "    reshaped = tf.reshape(embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n",
    "    reshaped = L.concatenate([reshaped, numerical_fea], axis=2)\n",
    "    \n",
    "    # RNN layer\n",
    "    # ここはループじゃなくて明示的に作ってるのか\n",
    "    if type == 0:\n",
    "        hidden = gru_layer(hidden_dim, dropout)(reshaped)\n",
    "        hidden = gru_layer(hidden_dim, dropout)(hidden)\n",
    "    elif type == 1:\n",
    "        hidden = lstm_layer(hidden_dim, dropout)(reshaped)\n",
    "        hidden = gru_layer(hidden_dim, dropout)(hidden)\n",
    "    elif type == 2:\n",
    "        hidden = gru_layer(hidden_dim, dropout)(reshaped)\n",
    "        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n",
    "    elif type == 3:\n",
    "        hidden = lstm_layer(hidden_dim, dropout)(reshaped)\n",
    "        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n",
    "    # truncated layer\n",
    "    truncated = hidden[:, :pred_len]\n",
    "    \n",
    "    # 出力\n",
    "    out = L.Dense(5, activation='linear')(truncated)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=out)\n",
    "    model.compile(tf.keras.optimizers.Adam(), loss=mcrmse)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('../data/train.json', lines=True)\n",
    "test = pd.read_json('../data/test.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional features\n",
    "\n",
    "# よくわからんけどデータ読み込んでる\n",
    "# データはすでに作ってるのか\n",
    "def read_bpps_sum(df):\n",
    "    bpps_arr = []\n",
    "    for mol_id in df.id.to_list():\n",
    "        bpps_arr.append(np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\").max(axis=1))\n",
    "    return bpps_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bpps_max(df):\n",
    "    bpps_arr = []\n",
    "    for mol_id in df.id.to_list():\n",
    "        bpps_arr.append(np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\").sum(axis=1))\n",
    "    return bpps_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bpps_nb(df):\n",
    "    # normalized non-zero number\n",
    "    # from https://www.kaggle.com/symyksr/openvaccine-deepergcn \n",
    "    bpps_nb_mean = 0.077522 # mean of bpps_nb across all training data\n",
    "    bpps_nb_std = 0.08914   # std of bpps_nb across all training data\n",
    "    bpps_arr = []\n",
    "    for mol_id in df.id.to_list():\n",
    "        bpps = np.load(f\"../input/stanford-covid-vaccine/bpps/{mol_id}.npy\")\n",
    "        bpps_nb = (bpps > 0).sum(axis=0) / bpps.shape[0]\n",
    "        bpps_nb = (bpps_nb - bpps_nb_mean) / bpps_nb_std\n",
    "        bpps_arr.append(bpps_nb)\n",
    "    return bpps_arr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量を作ってる。\n",
    "# 特徴量生成がある程度効果あるならBQで作るの良さそう。\n",
    "# まずはGPUかと思うけど\n",
    "train['bpps_sum'] = read_bpps_sum(train)\n",
    "test['bpps_sum'] = read_bpps_sum(test)\n",
    "train['bpps_max'] = read_bpps_max(train)\n",
    "test['bpps_max'] = read_bpps_max(test)\n",
    "train['bpps_nb'] = read_bpps_nb(train)\n",
    "test['bpps_nb'] = read_bpps_nb(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering for GroupKFold\n",
    "# expecting more accurate CV by putting similar RNAs into the same fold.\n",
    "kmeans_model = KMeans(n_clusters=200, random_state=110).fit(preprocess_inputs(train)[:,:,0])\n",
    "train['cluster_id'] = kmeans_model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation for training and TTA(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_df = pd.read_csv(aug_data)\n",
    "display(aug_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_data(df):\n",
    "    target_df = df.copy()\n",
    "    new_df = aug_df[aug_df['id'].isin(target_df['id'])]\n",
    "                         \n",
    "    del target_df['structure']\n",
    "    del target_df['predicted_loop_type']\n",
    "    new_df = new_df.merge(target_df, on=['id','sequence'], how='left')\n",
    "\n",
    "    df['cnt'] = df['id'].map(new_df[['id','cnt']].set_index('id').to_dict()['cnt'])\n",
    "    df['log_gamma'] = 100\n",
    "    df['score'] = 1.0\n",
    "    df = df.append(new_df[df.columns])\n",
    "    return df\n",
    "# あらかじめ作ってたデータをJOINしてる。\n",
    "# しかも新しい特徴量つくってる？？\n",
    "# 動かしてみないとなんともだけど、数合わない気がするなあ\n",
    "# あ、もはや既存のカラムは使ってないってこと？？\n",
    "train = aug_data(train)\n",
    "test = aug_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    train = train[:200]\n",
    "    test = test[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(type = 0, FOLD_N = 5):\n",
    "    \n",
    "    gkf = GroupKFold(n_splits=FOLD_N)\n",
    "\n",
    "    public_df = test.query(\"seq_length == 107\").copy()\n",
    "    private_df = test.query(\"seq_length == 130\").copy()\n",
    "\n",
    "    public_inputs = preprocess_inputs(public_df)\n",
    "    private_inputs = preprocess_inputs(private_df)\n",
    "\n",
    "\n",
    "    holdouts = []\n",
    "    holdout_preds = []\n",
    "\n",
    "    # 交差検証\n",
    "    for cv, (tr_idx, vl_idx) in enumerate(gkf.split(train,  train['reactivity'], train['cluster_id'])):\n",
    "        trn = train.iloc[tr_idx]\n",
    "        x_trn = preprocess_inputs(trn)\n",
    "        y_trn = np.array(trn[pred_cols].values.tolist()).transpose((0, 2, 1))\n",
    "        w_trn = np.log(trn.signal_to_noise+1.1)/2\n",
    "\n",
    "        val = train.iloc[vl_idx]\n",
    "        x_val_all = preprocess_inputs(val)\n",
    "        val = val[val.SN_filter == 1]\n",
    "        x_val = preprocess_inputs(val)\n",
    "        y_val = np.array(val[pred_cols].values.tolist()).transpose((0, 2, 1))\n",
    "\n",
    "        model = build_model(type=type)\n",
    "        model_short = build_model(seq_len=107, pred_len=107,type=type)\n",
    "        model_long = build_model(seq_len=130, pred_len=130,type=type)\n",
    "\n",
    "        history = model.fit(\n",
    "            x_trn, y_trn,\n",
    "            validation_data = (x_val, y_val),\n",
    "            batch_size=64,\n",
    "            epochs=60,\n",
    "            sample_weight=w_trn,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.ReduceLROnPlateau(),\n",
    "                tf.keras.callbacks.ModelCheckpoint(f'model{Ver}_cv{cv}.h5')\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        fig = px.line(\n",
    "            history.history, y=['loss', 'val_loss'], \n",
    "            labels={'index': 'epoch', 'value': 'Mean Squared Error'}, \n",
    "            title='Training History')\n",
    "        fig.show()\n",
    "\n",
    "        model.load_weights(f'model{Ver}_cv{cv}.h5')\n",
    "        model_short.load_weights(f'model{Ver}_cv{cv}.h5')\n",
    "        model_long.load_weights(f'model{Ver}_cv{cv}.h5')\n",
    "\n",
    "        holdouts.append(train.iloc[vl_idx])\n",
    "        holdout_preds.append(model.predict(x_val_all))\n",
    "        if cv == 0:\n",
    "            public_preds = model_short.predict(public_inputs)/FOLD_N\n",
    "            private_preds = model_long.predict(private_inputs)/FOLD_N\n",
    "        else:\n",
    "            public_preds += model_short.predict(public_inputs)/FOLD_N\n",
    "            private_preds += model_long.predict(private_inputs)/FOLD_N\n",
    "    return holdouts, holdout_preds, public_df, public_preds, private_df, private_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df, val_preds, test_df, test_preds = [], [], [], []\n",
    "if debug:\n",
    "    nmodel = 1\n",
    "else:\n",
    "    nmodel = 4\n",
    "for i in range(nmodel):\n",
    "    holdouts, holdout_preds, public_df, public_preds, private_df, private_preds = train_and_predict(i)\n",
    "    val_df += holdouts\n",
    "    val_preds += holdout_preds\n",
    "    test_df.append(public_df)\n",
    "    test_df.append(private_df)\n",
    "    test_preds.append(public_preds)\n",
    "    test_preds.append(private_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_ls = []\n",
    "for df, preds in zip(test_df, test_preds):\n",
    "    for i, uid in enumerate(df.id):\n",
    "        single_pred = preds[i]\n",
    "        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n",
    "        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
    "        preds_ls.append(single_df)\n",
    "preds_df = pd.concat(preds_ls).groupby('id_seqpos').mean().reset_index()\n",
    "# .mean() is for\n",
    "# 1, Predictions from multiple models\n",
    "# 2, TTA (augmented test data)\n",
    "\n",
    "preds_ls = []\n",
    "for df, preds in zip(val_df, val_preds):\n",
    "    for i, uid in enumerate(df.id):\n",
    "        single_pred = preds[i]\n",
    "        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n",
    "        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
    "        single_df['SN_filter'] = df[df['id'] == uid].SN_filter.values[0]\n",
    "        preds_ls.append(single_df)\n",
    "holdouts_df = pd.concat(preds_ls).groupby('id_seqpos').mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = preds_df[['id_seqpos', 'reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']]\n",
    "submission.to_csv(f'submission.csv', index=False)\n",
    "print(f'wrote to submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_mse(prd):\n",
    "    val = pd.read_json('../input/stanford-covid-vaccine/train.json', lines=True)\n",
    "\n",
    "    val_data = []\n",
    "    for mol_id in val['id'].unique():\n",
    "        sample_data = val.loc[val['id'] == mol_id]\n",
    "        sample_seq_length = sample_data.seq_length.values[0]\n",
    "        for i in range(68):\n",
    "            sample_dict = {\n",
    "                           'id_seqpos' : sample_data['id'].values[0] + '_' + str(i),\n",
    "                           'reactivity_gt' : sample_data['reactivity'].values[0][i],\n",
    "                           'deg_Mg_pH10_gt' : sample_data['deg_Mg_pH10'].values[0][i],\n",
    "                           'deg_Mg_50C_gt' : sample_data['deg_Mg_50C'].values[0][i],\n",
    "                           }\n",
    "            val_data.append(sample_dict)\n",
    "    val_data = pd.DataFrame(val_data)\n",
    "    val_data = val_data.merge(prd, on='id_seqpos')\n",
    "\n",
    "    rmses = []\n",
    "    mses = []\n",
    "    for col in ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']:\n",
    "        rmse = ((val_data[col] - val_data[col+'_gt']) ** 2).mean() ** .5\n",
    "        mse = ((val_data[col] - val_data[col+'_gt']) ** 2).mean()\n",
    "        rmses.append(rmse)\n",
    "        mses.append(mse)\n",
    "        print(col, rmse, mse)\n",
    "    print(np.mean(rmses), np.mean(mses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mse(holdouts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mse(holdouts_df[holdouts_df.SN_filter == 1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
