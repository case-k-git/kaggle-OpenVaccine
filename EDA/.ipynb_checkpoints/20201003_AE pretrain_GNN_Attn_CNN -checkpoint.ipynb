{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_dir = None # model dir for resuming training. if None, train from scrach\n",
    "\n",
    "one_fold = False # if True, train model at only first fold. use if you try a new idea quickly.\n",
    "run_test = False # if True, use small data. you can check whether this code run or not\n",
    "denoise = True # if True, use train data whose signal_to_noise > 1\n",
    "\n",
    "ae_epochs = 20 # epoch of training of denoising auto encoder\n",
    "ae_epochs_each = 5 # epoch of training of denoising auto encoder each time. \n",
    "                   # I use train data (seqlen = 107) and private test data (seqlen = 130) for auto encoder training.\n",
    "                   # I dont know how to easily fit keras model to use both of different shape data simultaneously, \n",
    "                   # so I call fit function several times. \n",
    "ae_batch_size = 32\n",
    "\n",
    "epochs_list = [30, 10, 3, 3, 5, 5]\n",
    "batch_size_list = [8, 16, 32, 64, 128, 256] \n",
    "\n",
    "## copy pretrain model to working dir\n",
    "import shutil\n",
    "import glob\n",
    "if pretrain_dir is not None:\n",
    "    for d in glob.glob(pretrain_dir + \"*\"):\n",
    "        shutil.copy(d, \".\")\n",
    "    \n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "train = pd.read_json(\"../data/train.json\",lines=True)\n",
    "if denoise:\n",
    "    train = train[train.signal_to_noise > 1].reset_index(drop = True)\n",
    "test  = pd.read_json(\"../data/test.json\",lines=True)\n",
    "# seq_lengthで分けてるのか\n",
    "test_pub = test[test[\"seq_length\"] == 107]\n",
    "test_pri = test[test[\"seq_length\"] == 130]\n",
    "sub = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "\n",
    "if run_test: ## to test \n",
    "    train = train[:30]\n",
    "    test_pub = test_pub[:30]\n",
    "    test_pri = test_pri[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2096, 19)\n",
      "(3634, 7)\n",
      "(629, 7)\n",
      "(3005, 7)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(test_pub.shape)\n",
    "print(test_pri.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86040ad774d341d2bde897d8c2469092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2096.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7523ead59d14d9fb32e3baa5955f646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=629.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db8f09e50da4feb87ee26f8c72fbdd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3005.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "As = []\n",
    "# read bpps\n",
    "for id in tqdm(train[\"id\"]):\n",
    "    a = np.load(f\"../data/bpps/{id}.npy\")\n",
    "    As.append(a)\n",
    "As = np.array(As)\n",
    "As_pub = []\n",
    "for id in tqdm(test_pub[\"id\"]):\n",
    "    a = np.load(f\"../data/bpps/{id}.npy\")\n",
    "    As_pub.append(a)\n",
    "As_pub = np.array(As_pub)\n",
    "As_pri = []\n",
    "for id in tqdm(test_pri[\"id\"]):\n",
    "    a = np.load(f\"../data/bpps/{id}.npy\")\n",
    "    As_pri.append(a)\n",
    "As_pri = np.array(As_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2096, 107, 107)\n",
      "(629, 107, 107)\n",
      "(3005, 130, 130)\n"
     ]
    }
   ],
   "source": [
    "print(As.shape)\n",
    "print(As_pub.shape)\n",
    "print(As_pri.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2096, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>structure</th>\n",
       "      <th>predicted_loop_type</th>\n",
       "      <th>signal_to_noise</th>\n",
       "      <th>SN_filter</th>\n",
       "      <th>seq_length</th>\n",
       "      <th>seq_scored</th>\n",
       "      <th>reactivity_error</th>\n",
       "      <th>deg_error_Mg_pH10</th>\n",
       "      <th>deg_error_pH10</th>\n",
       "      <th>deg_error_Mg_50C</th>\n",
       "      <th>deg_error_50C</th>\n",
       "      <th>reactivity</th>\n",
       "      <th>deg_Mg_pH10</th>\n",
       "      <th>deg_pH10</th>\n",
       "      <th>deg_Mg_50C</th>\n",
       "      <th>deg_50C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id_001f94081</td>\n",
       "      <td>GGAAAAGCUCUAAUAACAGGAGACUAGGACUACGUAUUUCUAGGUA...</td>\n",
       "      <td>.....((((((.......)))).)).((.....((..((((((......</td>\n",
       "      <td>EEEEESSSSSSHHHHHHHSSSSBSSXSSIIIIISSIISSSSSSHHH...</td>\n",
       "      <td>6.894</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "      <td>68</td>\n",
       "      <td>[0.1359, 0.20700000000000002, 0.1633, 0.1452, ...</td>\n",
       "      <td>[0.26130000000000003, 0.38420000000000004, 0.1...</td>\n",
       "      <td>[0.2631, 0.28600000000000003, 0.0964, 0.1574, ...</td>\n",
       "      <td>[0.1501, 0.275, 0.0947, 0.18660000000000002, 0...</td>\n",
       "      <td>[0.2167, 0.34750000000000003, 0.188, 0.2124, 0...</td>\n",
       "      <td>[0.3297, 1.5693000000000001, 1.1227, 0.8686, 0...</td>\n",
       "      <td>[0.7556, 2.983, 0.2526, 1.3789, 0.637600000000...</td>\n",
       "      <td>[2.3375, 3.5060000000000002, 0.3008, 1.0108, 0...</td>\n",
       "      <td>[0.35810000000000003, 2.9683, 0.2589, 1.4552, ...</td>\n",
       "      <td>[0.6382, 3.4773, 0.9988, 1.3228, 0.78770000000...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index            id                                           sequence  \\\n",
       "0      0  id_001f94081  GGAAAAGCUCUAAUAACAGGAGACUAGGACUACGUAUUUCUAGGUA...   \n",
       "\n",
       "                                           structure  \\\n",
       "0  .....((((((.......)))).)).((.....((..((((((......   \n",
       "\n",
       "                                 predicted_loop_type  signal_to_noise  \\\n",
       "0  EEEEESSSSSSHHHHHHHSSSSBSSXSSIIIIISSIISSSSSSHHH...            6.894   \n",
       "\n",
       "   SN_filter  seq_length  seq_scored  \\\n",
       "0          1         107          68   \n",
       "\n",
       "                                    reactivity_error  \\\n",
       "0  [0.1359, 0.20700000000000002, 0.1633, 0.1452, ...   \n",
       "\n",
       "                                   deg_error_Mg_pH10  \\\n",
       "0  [0.26130000000000003, 0.38420000000000004, 0.1...   \n",
       "\n",
       "                                      deg_error_pH10  \\\n",
       "0  [0.2631, 0.28600000000000003, 0.0964, 0.1574, ...   \n",
       "\n",
       "                                    deg_error_Mg_50C  \\\n",
       "0  [0.1501, 0.275, 0.0947, 0.18660000000000002, 0...   \n",
       "\n",
       "                                       deg_error_50C  \\\n",
       "0  [0.2167, 0.34750000000000003, 0.188, 0.2124, 0...   \n",
       "\n",
       "                                          reactivity  \\\n",
       "0  [0.3297, 1.5693000000000001, 1.1227, 0.8686, 0...   \n",
       "\n",
       "                                         deg_Mg_pH10  \\\n",
       "0  [0.7556, 2.983, 0.2526, 1.3789, 0.637600000000...   \n",
       "\n",
       "                                            deg_pH10  \\\n",
       "0  [2.3375, 3.5060000000000002, 0.3008, 1.0108, 0...   \n",
       "\n",
       "                                          deg_Mg_50C  \\\n",
       "0  [0.35810000000000003, 2.9683, 0.2589, 1.4552, ...   \n",
       "\n",
       "                                             deg_50C  \n",
       "0  [0.6382, 3.4773, 0.9988, 1.3228, 0.78770000000...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train.shape)\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3634, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>structure</th>\n",
       "      <th>predicted_loop_type</th>\n",
       "      <th>seq_length</th>\n",
       "      <th>seq_scored</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>id_00073f8be</td>\n",
       "      <td>GGAAAAGUACGACUUGAGUACGGAAAACGUACCAACUCGAUUAAAA...</td>\n",
       "      <td>......((((((((((.(((((.....))))))))((((((((......</td>\n",
       "      <td>EEEEEESSSSSSSSSSBSSSSSHHHHHSSSSSSSSSSSSSSSSHHH...</td>\n",
       "      <td>107</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index            id                                           sequence  \\\n",
       "0      0  id_00073f8be  GGAAAAGUACGACUUGAGUACGGAAAACGUACCAACUCGAUUAAAA...   \n",
       "\n",
       "                                           structure  \\\n",
       "0  ......((((((((((.(((((.....))))))))((((((((......   \n",
       "\n",
       "                                 predicted_loop_type  seq_length  seq_scored  \n",
       "0  EEEEEESSSSSSSSSSBSSSSSHHHHHSSSSSSSSSSSSSSSSHHH...         107          68  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test.shape)\n",
    "test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(457953, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_seqpos</th>\n",
       "      <th>reactivity</th>\n",
       "      <th>deg_Mg_pH10</th>\n",
       "      <th>deg_pH10</th>\n",
       "      <th>deg_Mg_50C</th>\n",
       "      <th>deg_50C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_00073f8be_0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id_seqpos  reactivity  deg_Mg_pH10  deg_pH10  deg_Mg_50C  deg_50C\n",
       "0  id_00073f8be_0         0.0          0.0       0.0         0.0      0.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sub.shape)\n",
    "sub.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n",
      "107\n",
      "68\n",
      "39\n",
      "(5, 2096, 107)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2096, 107, 5)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = list(sub.columns[1:])\n",
    "print(targets)\n",
    "\n",
    "y_train = []\n",
    "seq_len = train[\"seq_length\"].iloc[0]\n",
    "seq_len_target = train[\"seq_scored\"].iloc[0]\n",
    "print(seq_len)\n",
    "print(seq_len_target)\n",
    "\n",
    "ignore = -10000\n",
    "ignore_length = seq_len - seq_len_target\n",
    "print(ignore_length)\n",
    "\n",
    "for target in targets:\n",
    "    y = np.vstack(train[target]) #縦方向に結合  (2096, 68)\n",
    "    # (2096, 39)、スカラ0のテンソルをつくり、値をignore( -10000)\n",
    "    dummy = np.zeros([y.shape[0], ignore_length]) + ignore # (2096, 39)\n",
    "    y = np.hstack([y, dummy]) # 横方向に結合 39 + 68 = 108\n",
    "    y_train.append(y)\n",
    "print(np.array(y_train).shape) # (5, 2096, 107) (カラム, 行, 列)\n",
    "y = np.stack(y_train, axis = 2) # (2096, 107, 5) 2次元めに沿って結合\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.2970e-01  7.5560e-01  2.3375e+00  3.5810e-01  6.3820e-01]\n",
      " [ 1.5693e+00  2.9830e+00  3.5060e+00  2.9683e+00  3.4773e+00]\n",
      " [ 1.1227e+00  2.5260e-01  3.0080e-01  2.5890e-01  9.9880e-01]\n",
      " [ 8.6860e-01  1.3789e+00  1.0108e+00  1.4552e+00  1.3228e+00]\n",
      " [ 7.2170e-01  6.3760e-01  2.6350e-01  7.2440e-01  7.8770e-01]\n",
      " [ 4.3840e-01  3.3130e-01  3.4030e-01  4.9710e-01  5.8900e-01]\n",
      " [ 2.5600e-01  6.7630e-01  5.6170e-01  3.8190e-01  5.2310e-01]\n",
      " [ 3.3640e-01  7.5250e-01  6.8100e-01  9.1150e-01  1.0125e+00]\n",
      " [ 2.1680e-01  2.0800e-01  5.3000e-02  1.6680e-01  5.8500e-02]\n",
      " [ 3.5830e-01  8.9080e-01  5.3480e-01  1.0613e+00  7.3490e-01]\n",
      " [ 9.5410e-01  9.8980e-01  5.5260e-01  1.1317e+00  1.2215e+00]\n",
      " [ 1.4113e+00  8.6790e-01  3.2780e-01  7.4680e-01  1.0321e+00]\n",
      " [ 1.6911e+00  1.7403e+00  1.0806e+00  1.9049e+00  8.2740e-01]\n",
      " [ 1.2494e+00  1.3795e+00  7.6990e-01  1.1030e+00  1.0442e+00]\n",
      " [ 1.1895e+00  7.7460e-01  3.9010e-01  7.3170e-01  8.4720e-01]\n",
      " [ 6.9090e-01  7.1900e-01  5.0450e-01  7.6490e-01  5.5470e-01]\n",
      " [ 4.7360e-01  7.2290e-01  6.4750e-01  7.0100e-01  7.2910e-01]\n",
      " [ 1.7540e-01  4.0690e-01  5.1760e-01  4.3270e-01  5.5410e-01]\n",
      " [ 5.8200e-02  2.6320e-01  2.2060e-01  2.3600e-01  2.4040e-01]\n",
      " [ 2.1730e-01  3.6310e-01  2.2550e-01  4.0390e-01  3.5980e-01]\n",
      " [ 7.8500e-02  7.0200e-02  5.8100e-02  3.7300e-02  1.4960e-01]\n",
      " [ 8.2490e-01  4.7650e-01  4.3020e-01  3.3620e-01  6.1540e-01]\n",
      " [ 7.6380e-01  2.7660e-01  3.3440e-01  2.9140e-01  4.0530e-01]\n",
      " [ 1.0950e-01  4.9360e-01  2.7180e-01  3.9560e-01  4.6410e-01]\n",
      " [ 2.5680e-01  4.3790e-01  4.6600e-01  3.0740e-01  5.9760e-01]\n",
      " [ 8.9500e-02  3.2610e-01  4.7630e-01  3.0790e-01  6.7470e-01]\n",
      " [ 1.5760e-01  8.9800e-02  1.2750e-01  2.3790e-01  2.9790e-01]\n",
      " [ 7.7270e-01  4.1590e-01  6.8860e-01  2.0270e-01  8.3760e-01]\n",
      " [ 1.5730e-01  2.2640e-01  2.3230e-01  3.7210e-01  3.9150e-01]\n",
      " [ 5.0430e-01  4.1420e-01  1.1860e-01  4.5040e-01  4.2260e-01]\n",
      " [ 1.0444e+00  5.2130e-01  8.3210e-01  6.3030e-01  7.2230e-01]\n",
      " [ 4.7660e-01  2.6530e-01  1.3170e-01  2.0920e-01  2.9910e-01]\n",
      " [ 5.5880e-01  3.1630e-01  5.8780e-01  6.2190e-01  9.4970e-01]\n",
      " [ 9.0540e-01  9.0590e-01  7.1620e-01  1.0619e+00  1.2491e+00]\n",
      " [ 1.0125e+00  8.7620e-01  7.9430e-01  8.6670e-01  1.0324e+00]\n",
      " [ 1.0482e+00  8.9910e-01  7.2240e-01  9.2060e-01  7.8680e-01]\n",
      " [ 1.0440e+00  7.2620e-01  8.1050e-01  8.5160e-01  1.0355e+00]\n",
      " [ 4.5220e-01  4.7620e-01  4.7820e-01  6.3080e-01  6.6040e-01]\n",
      " [ 2.1100e-01  1.1870e-01  1.7970e-01  1.7560e-01  1.9210e-01]\n",
      " [ 4.6100e-02  1.9520e-01  1.3440e-01  4.0220e-01  3.5930e-01]\n",
      " [ 8.2000e-02  1.7770e-01  2.2170e-01  2.2360e-01  3.7990e-01]\n",
      " [ 6.4300e-02  1.2810e-01  1.1520e-01  1.4520e-01  1.5100e-01]\n",
      " [ 1.5260e-01  2.6640e-01  1.7070e-01  1.4690e-01  2.6360e-01]\n",
      " [ 8.9400e-02  8.8310e-01  4.3240e-01  6.6220e-01  4.9190e-01]\n",
      " [ 5.0810e-01  8.7520e-01  4.1660e-01  5.6920e-01  7.4860e-01]\n",
      " [ 1.0745e+00  8.6930e-01  4.5580e-01  4.5830e-01  6.3040e-01]\n",
      " [ 3.2150e-01  5.1190e-01  3.6060e-01  2.7050e-01  3.4660e-01]\n",
      " [ 7.1600e-02  3.4760e-01  2.2390e-01  2.7980e-01  3.0330e-01]\n",
      " [ 2.4400e-02  4.3800e-02  4.8100e-02  2.9800e-02  8.5500e-02]\n",
      " [ 1.2300e-02  1.2360e-01  6.9100e-02  3.6700e-02  1.3870e-01]\n",
      " [ 1.9840e-01  4.5800e-01  2.4230e-01  2.2990e-01  4.2540e-01]\n",
      " [ 4.9610e-01  3.2140e-01  1.0030e-01  2.8730e-01  3.5710e-01]\n",
      " [ 1.0641e+00  2.1155e+00  9.3220e-01  1.7464e+00  7.9400e-01]\n",
      " [ 6.3940e-01  3.7080e-01  2.5300e-01  3.2330e-01  2.2580e-01]\n",
      " [ 6.7890e-01  3.5360e-01  2.9230e-01  2.9960e-01  3.4570e-01]\n",
      " [ 3.6500e-01  2.6690e-01  1.4550e-01  2.0290e-01  1.5140e-01]\n",
      " [ 1.7410e-01  3.1610e-01  2.5230e-01  2.3110e-01  1.6020e-01]\n",
      " [ 1.4080e-01  1.7930e-01  2.5040e-01  2.0320e-01  1.5720e-01]\n",
      " [ 1.6460e-01  3.2590e-01  4.0650e-01  4.0790e-01  3.9120e-01]\n",
      " [ 5.3890e-01  4.7000e-01  7.3610e-01  5.5440e-01  3.7240e-01]\n",
      " [ 6.8300e-01  6.1410e-01  9.4120e-01  3.8290e-01  4.4320e-01]\n",
      " [ 4.2730e-01  3.1220e-01  3.1370e-01  3.4450e-01  2.1690e-01]\n",
      " [ 5.2700e-02  1.4180e-01  3.4890e-01  1.8980e-01  1.9590e-01]\n",
      " [ 6.9300e-02  1.2770e-01  4.1400e-01  2.6490e-01  2.3910e-01]\n",
      " [ 1.3980e-01  1.6080e-01  2.2380e-01  2.7140e-01  4.2340e-01]\n",
      " [ 2.9370e-01  3.3360e-01  5.1420e-01  4.8120e-01  3.2870e-01]\n",
      " [ 2.3620e-01  6.4910e-01  7.6810e-01  7.0260e-01  5.3010e-01]\n",
      " [ 5.7310e-01  6.8980e-01  1.1720e+00  4.2540e-01  8.4720e-01]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]\n",
      " [-1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04 -1.0000e+04]]\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### structure adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_structure_adj(train):\n",
    "    ## get adjacent matrix from structure sequence\n",
    "    \n",
    "    ## here I calculate adjacent matrix of each base pair, \n",
    "    ## but eventually ignore difference of base pair and integrate into one matrix\n",
    "    Ss = []\n",
    "    for i in tqdm(range(len(train))):\n",
    "        seq_length = train[\"seq_length\"].iloc[i]\n",
    "        structure = train[\"structure\"].iloc[i]\n",
    "        sequence = train[\"sequence\"].iloc[i]\n",
    "\n",
    "        cue = []\n",
    "        # ０？\n",
    "        a_structures = {\n",
    "            (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n",
    "            (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n",
    "        }\n",
    "        a_structure = np.zeros([seq_length, seq_length])\n",
    "        for i in range(seq_length):\n",
    "            if structure[i] == \"(\":\n",
    "                cue.append(i)\n",
    "            elif structure[i] == \")\":\n",
    "                start = cue.pop()\n",
    "#                 a_structure[start, i] = 1\n",
    "#                 a_structure[i, start] = 1\n",
    "                a_structures[(sequence[start], sequence[i])][start, i] = 1\n",
    "                a_structures[(sequence[i], sequence[start])][i, start] = 1\n",
    "        \n",
    "        a_strc = np.stack([a for a in a_structures.values()], axis = 2)\n",
    "        a_strc = np.sum(a_strc, axis = 2, keepdims = True)\n",
    "        Ss.append(a_strc)\n",
    "    \n",
    "    Ss = np.array(Ss)\n",
    "    print(Ss.shape)\n",
    "    return Ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7254141424e347bf8e7009b16bba63ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2096.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(2096, 107, 107, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f50be509f574b5495d4cc40b16d03c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=629.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(629, 107, 107, 1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b884224c0c0e4e1b9f4200717f273df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3005.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(3005, 130, 130, 1)\n"
     ]
    }
   ],
   "source": [
    "Ss = get_structure_adj(train)\n",
    "Ss_pub = get_structure_adj(test_pub)\n",
    "Ss_pri = get_structure_adj(test_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2096, 107, 107, 1)\n",
      "(629, 107, 107, 1)\n",
      "(3005, 130, 130, 1)\n"
     ]
    }
   ],
   "source": [
    "# 空のテンソルかな\n",
    "print(Ss.shape)\n",
    "print(Ss_pub.shape)\n",
    "print(Ss_pri.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distance adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance_matrix(As):\n",
    "    ## adjacent matrix based on distance on the sequence\n",
    "    ## D[i, j] = 1 / (abs(i - j) + 1) ** pow, pow = 1, 2, 4\n",
    "    \n",
    "    idx = np.arange(As.shape[1])\n",
    "    Ds = []\n",
    "    for i in range(len(idx)):\n",
    "        d = np.abs(idx[i] - idx)\n",
    "        Ds.append(d)\n",
    "\n",
    "    Ds = np.array(Ds) + 1\n",
    "    Ds = 1/Ds\n",
    "    Ds = Ds[None, :,:]\n",
    "    Ds = np.repeat(Ds, len(As), axis = 0)\n",
    "    \n",
    "    Dss = []\n",
    "    for i in [1, 2, 4]: \n",
    "        Dss.append(Ds ** i)\n",
    "    Ds = np.stack(Dss, axis = 3)\n",
    "    print(Ds.shape)\n",
    "    return Ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2096, 107, 107, 3)\n",
      "(629, 107, 107, 3)\n",
      "(3005, 130, 130, 3)\n"
     ]
    }
   ],
   "source": [
    "Ds = get_distance_matrix(As)\n",
    "Ds_pub = get_distance_matrix(As_pub)\n",
    "Ds_pri = get_distance_matrix(As_pri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2096, 107, 107, 5), (629, 107, 107, 5), (3005, 130, 130, 5))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## concat adjecent\n",
    "As = np.concatenate([As[:,:,:,None], Ss, Ds], axis = 3).astype(np.float32)\n",
    "As_pub = np.concatenate([As_pub[:,:,:,None], Ss_pub, Ds_pub], axis = 3).astype(np.float32)\n",
    "As_pri = np.concatenate([As_pri[:,:,:,None], Ss_pri, Ds_pri], axis = 3).astype(np.float32)\n",
    "del Ss, Ds, Ss_pub, Ds_pub, Ss_pri, Ds_pri\n",
    "As.shape, As_pub.shape, As_pri.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 18, 20, 24, 33, 34, 36, 40, 65, 66, 68, 72, 129, 130, 132, 136, 257, 258, 260, 264, 513, 514, 516, 520, 1025, 1026, 1028, 1032]\n",
      "(2096, 107, 39)\n",
      "[17, 18, 20, 24, 33, 34, 36, 40, 65, 66, 68, 72, 129, 130, 132, 136, 257, 258, 260, 264, 513, 514, 516, 520, 1025, 1026, 1028, 1032]\n",
      "(629, 107, 39)\n",
      "[17, 18, 20, 24, 33, 34, 36, 40, 65, 66, 68, 72, 129, 130, 132, 136, 257, 258, 260, 264, 513, 514, 516, 520, 1025, 1026, 1028, 1032]\n",
      "(3005, 130, 39)\n"
     ]
    }
   ],
   "source": [
    "## sequence\n",
    "def return_ohe(n, i):\n",
    "    tmp = [0] * n\n",
    "    tmp[i] = 1\n",
    "    return tmp\n",
    "\n",
    "def get_input(train):\n",
    "    ## get node features, which is one hot encoded\n",
    "    mapping = {}\n",
    "    vocab = [\"A\", \"G\", \"C\", \"U\"]\n",
    "    for i, s in enumerate(vocab):\n",
    "        mapping[s] = return_ohe(len(vocab), i)\n",
    "    X_node = np.stack(train[\"sequence\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n",
    "\n",
    "    mapping = {}\n",
    "    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n",
    "    for i, s in enumerate(vocab):\n",
    "        mapping[s] = return_ohe(len(vocab), i)\n",
    "    X_loop = np.stack(train[\"predicted_loop_type\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n",
    "    \n",
    "    mapping = {}\n",
    "    vocab = [\".\", \"(\", \")\"]\n",
    "    for i, s in enumerate(vocab):\n",
    "        mapping[s] = return_ohe(len(vocab), i)\n",
    "    X_structure = np.stack(train[\"structure\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n",
    "    \n",
    "    \n",
    "    X_node = np.concatenate([X_node, X_loop], axis = 2)\n",
    "    \n",
    "    ## interaction\n",
    "    a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis = 2)\n",
    "    vocab = sorted(set(a.flatten()))\n",
    "    print(vocab)\n",
    "    ohes = []\n",
    "    for v in vocab:\n",
    "        ohes.append(a == v)\n",
    "    ohes = np.stack(ohes, axis = 2)\n",
    "    X_node = np.concatenate([X_node, ohes], axis = 2).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    print(X_node.shape)\n",
    "    return X_node\n",
    "\n",
    "X_node = get_input(train)\n",
    "X_node_pub = get_input(test_pub)\n",
    "X_node_pri = get_input(test_pri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.3.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers as L\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def mcrmse(t, p, seq_len_target = seq_len_target):\n",
    "    ## calculate mcrmse score by using numpy\n",
    "    t = t[:, :seq_len_target]\n",
    "    p = p[:, :seq_len_target]\n",
    "    \n",
    "    score = np.mean(np.sqrt(np.mean(np.mean((p - t) ** 2, axis = 1), axis = 0)))\n",
    "    return score\n",
    "\n",
    "def mcrmse_loss(t, y, seq_len_target = seq_len_target):\n",
    "    ## calculate mcrmse score by using tf\n",
    "    t = t[:, :seq_len_target]\n",
    "    y = y[:, :seq_len_target]\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.sqrt(tf.reduce_mean(tf.reduce_mean((t - y) ** 2, axis = 1), axis = 0)))\n",
    "    return loss\n",
    "\n",
    "def attention(x_inner, x_outer, n_factor, dropout):\n",
    "    x_Q =  L.Conv1D(n_factor, 1, activation='linear', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(x_inner)\n",
    "    x_K =  L.Conv1D(n_factor, 1, activation='linear', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(x_outer)\n",
    "    x_V =  L.Conv1D(n_factor, 1, activation='linear', \n",
    "                  kernel_initializer='glorot_uniform',\n",
    "                  bias_initializer='glorot_uniform',\n",
    "                 )(x_outer)\n",
    "    x_KT = L.Permute((2, 1))(x_K)\n",
    "    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) / np.sqrt(n_factor))([x_Q, x_KT])\n",
    "#     res = tf.expand_dims(res, axis = 3)\n",
    "#     res = L.Conv2D(16, 3, 1, padding = \"same\", activation = \"relu\")(res)\n",
    "#     res = L.Conv2D(1, 3, 1, padding = \"same\", activation = \"relu\")(res)\n",
    "#     res = tf.squeeze(res, axis = 3)\n",
    "    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n",
    "    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n",
    "    return att\n",
    "\n",
    "def multi_head_attention(x, y, n_factor, n_head, dropout):\n",
    "    if n_head == 1:\n",
    "        att = attention(x, y, n_factor, dropout)\n",
    "    else:\n",
    "        n_factor_head = n_factor // n_head\n",
    "        heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n",
    "        att = L.Concatenate()(heads)\n",
    "        att = L.Dense(n_factor, \n",
    "                      kernel_initializer='glorot_uniform',\n",
    "                      bias_initializer='glorot_uniform',\n",
    "                     )(att)\n",
    "    x = L.Add()([x, att])\n",
    "    x = L.LayerNormalization()(x)\n",
    "    if dropout > 0:\n",
    "        x = L.Dropout(dropout)(x)\n",
    "    return x\n",
    "\n",
    "def res(x, unit, kernel = 3, rate = 0.1):\n",
    "    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n",
    "    h = L.LayerNormalization()(h)\n",
    "    h = L.LeakyReLU()(h)\n",
    "    h = L.Dropout(rate)(h)\n",
    "    return L.Add()([x, h])\n",
    "\n",
    "def forward(x, unit, kernel = 3, rate = 0.1):\n",
    "#     h = L.Dense(unit, None)(x)\n",
    "    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n",
    "    h = L.LayerNormalization()(h)\n",
    "    h = L.Dropout(rate)(h)\n",
    "#         h = tf.keras.activations.swish(h)\n",
    "    h = L.LeakyReLU()(h)\n",
    "    h = res(h, unit, kernel, rate)\n",
    "    return h\n",
    "\n",
    "def adj_attn(x, adj, unit, n = 2, rate = 0.1):\n",
    "    x_a = x\n",
    "    x_as = []\n",
    "    for i in range(n):\n",
    "        x_a = forward(x_a, unit)\n",
    "        x_a = tf.matmul(adj, x_a) ## aggregate neighborhoods\n",
    "        x_as.append(x_a)\n",
    "    if n == 1:\n",
    "        x_a = x_as[0]\n",
    "    else:\n",
    "        x_a = L.Concatenate()(x_as)\n",
    "    x_a = forward(x_a, unit)\n",
    "    return x_a\n",
    "\n",
    "\n",
    "def get_base(config):\n",
    "    ## base model architecture \n",
    "    ## node, adj -> middle feature\n",
    "    \n",
    "    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n",
    "    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n",
    "    \n",
    "    adj_learned = L.Dense(1, \"relu\")(adj)\n",
    "    adj_all = L.Concatenate(axis = 3)([adj, adj_learned])\n",
    "        \n",
    "    xs = []\n",
    "    xs.append(node)\n",
    "    x1 = forward(node, 128, kernel = 3, rate = 0.0)\n",
    "    x2 = forward(x1, 64, kernel = 6, rate = 0.0)\n",
    "    x3 = forward(x2, 32, kernel = 15, rate = 0.0)\n",
    "    x4 = forward(x3, 16, kernel = 30, rate = 0.0)\n",
    "    x = L.Concatenate()([x1, x2, x3, x4])\n",
    "    \n",
    "    for unit in [64, 32]:\n",
    "        x_as = []\n",
    "        for i in range(adj_all.shape[3]):\n",
    "            x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate = 0.0)\n",
    "            x_as.append(x_a)\n",
    "        x_c = forward(x, unit, kernel = 30)\n",
    "        \n",
    "        x = L.Concatenate()(x_as + [x_c])\n",
    "        x = forward(x, unit)\n",
    "        x = multi_head_attention(x, x, unit, 4, 0.0)\n",
    "        xs.append(x)\n",
    "        \n",
    "    x = L.Concatenate()(xs)\n",
    "\n",
    "    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_ae_model(base, config):\n",
    "    ## denoising auto encoder part\n",
    "    ## node, adj -> middle feature -> node\n",
    "    \n",
    "    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n",
    "    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n",
    "\n",
    "    x = base([L.SpatialDropout1D(0.3)(node), adj])\n",
    "    x = forward(x, 64, rate = 0.3)\n",
    "    p = L.Dense(X_node.shape[2], \"sigmoid\")(x)\n",
    "    \n",
    "    loss = - tf.reduce_mean(20 * node * tf.math.log(p + 1e-4) + (1 - node) * tf.math.log(1 - p + 1e-4))\n",
    "    model = tf.keras.Model(inputs = [node, adj], outputs = [loss])\n",
    "    \n",
    "    opt = get_optimizer()\n",
    "    model.compile(optimizer = opt, loss = lambda t, y : y)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model(base, config):\n",
    "    ## regression part\n",
    "    ## node, adj -> middle feature -> prediction of targets\n",
    "    \n",
    "    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n",
    "    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n",
    "    \n",
    "    x = base([node, adj])\n",
    "    x = forward(x, 128, rate = 0.4)\n",
    "    x = L.Dense(5, None)(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n",
    "    \n",
    "    opt = get_optimizer()\n",
    "    model.compile(optimizer = opt, loss = mcrmse_loss)\n",
    "    return model\n",
    "\n",
    "def get_optimizer():\n",
    "#     sgd = tf.keras.optimizers.SGD(0.05, momentum = 0.9, nesterov=True)\n",
    "    adam = tf.optimizers.Adam()\n",
    "#     radam = tfa.optimizers.RectifiedAdam()\n",
    "#     lookahead = tfa.optimizers.Lookahead(adam, sync_period=6)\n",
    "#     swa = tfa.optimizers.SWA(adam)\n",
    "    return adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrain\n",
    "```\n",
    "start:15:31\n",
    "end:15:38\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ 0 ------\n",
      "--- train ---\n",
      "Epoch 1/5\n",
      "66/66 [==============================] - 4s 56ms/step - loss: 0.9215\n",
      "Epoch 2/5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.3174\n",
      "Epoch 3/5\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.1643\n",
      "Epoch 4/5\n",
      "66/66 [==============================] - 3s 53ms/step - loss: 0.1078\n",
      "Epoch 5/5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.0793\n",
      "--- public ---\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 1s 74ms/step - loss: 0.0722\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 0.0630\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 0.0576\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 0.0566\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 0.0509\n",
      "--- private ---\n",
      "Epoch 1/5\n",
      "94/94 [==============================] - 6s 62ms/step - loss: 0.0570\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 5s 55ms/step - loss: 0.0445\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 5s 56ms/step - loss: 0.0368\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 5s 54ms/step - loss: 0.0322\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 5s 56ms/step - loss: 0.0343\n",
      "------ 1 ------\n",
      "--- train ---\n",
      "Epoch 1/5\n",
      "66/66 [==============================] - 3s 48ms/step - loss: 0.0240\n",
      "Epoch 2/5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.0227\n",
      "Epoch 3/5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.0215\n",
      "Epoch 4/5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.0245\n",
      "Epoch 5/5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.0191\n",
      "--- public ---\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.0189\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 0.0194\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.0193\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 0.0168\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 1s 48ms/step - loss: 0.0148\n",
      "--- private ---\n",
      "Epoch 1/5\n",
      "94/94 [==============================] - 5s 54ms/step - loss: 0.0203\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 5s 55ms/step - loss: 0.0199\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 5s 56ms/step - loss: 0.0160\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 5s 55ms/step - loss: 0.0174\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 5s 55ms/step - loss: 0.0144\n",
      "------ 2 ------\n",
      "--- train ---\n",
      "Epoch 1/5\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.0146\n",
      "Epoch 2/5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.0135\n",
      "Epoch 3/5\n",
      "66/66 [==============================] - 3s 49ms/step - loss: 0.0113\n",
      "Epoch 4/5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.0168\n",
      "Epoch 5/5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.0123\n",
      "--- public ---\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 1s 49ms/step - loss: 0.0123\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 1s 51ms/step - loss: 0.0098\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 0.0113\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 1s 46ms/step - loss: 0.0156\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 0.0154\n",
      "--- private ---\n",
      "Epoch 1/5\n",
      "94/94 [==============================] - 5s 54ms/step - loss: 0.0143\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 5s 54ms/step - loss: 0.0120\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 5s 55ms/step - loss: 0.0111\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 5s 55ms/step - loss: 0.0098\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 5s 55ms/step - loss: 0.0112\n",
      "------ 3 ------\n",
      "--- train ---\n",
      "Epoch 1/5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.0094\n",
      "Epoch 2/5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.0096\n",
      "Epoch 3/5\n",
      "66/66 [==============================] - 3s 50ms/step - loss: 0.0153\n",
      "Epoch 4/5\n",
      "66/66 [==============================] - 3s 51ms/step - loss: 0.0085\n",
      "Epoch 5/5\n",
      "66/66 [==============================] - 3s 52ms/step - loss: 0.0082\n",
      "--- public ---\n",
      "Epoch 1/5\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.0113\n",
      "Epoch 2/5\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 0.0084\n",
      "Epoch 3/5\n",
      "20/20 [==============================] - 1s 53ms/step - loss: 0.0073\n",
      "Epoch 4/5\n",
      "20/20 [==============================] - 1s 50ms/step - loss: 0.0076\n",
      "Epoch 5/5\n",
      "20/20 [==============================] - 1s 47ms/step - loss: 0.0091\n",
      "--- private ---\n",
      "Epoch 1/5\n",
      "94/94 [==============================] - 5s 55ms/step - loss: 0.0105\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 5s 54ms/step - loss: 0.0106\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 5s 54ms/step - loss: 0.0096\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 5s 55ms/step - loss: 0.0080\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 5s 55ms/step - loss: 0.0084\n",
      "****** save ae model ******\n"
     ]
    }
   ],
   "source": [
    "## here train denoising auto encoder model using all data\n",
    "\n",
    "config = {} ## not use now\n",
    "if ae_epochs > 0:\n",
    "    base = get_base(config)\n",
    "    ae_model = get_ae_model(base, config)\n",
    "    ## TODO : simultaneous train\n",
    "    for i in range(ae_epochs//ae_epochs_each):\n",
    "        print(f\"------ {i} ------\")\n",
    "        print(\"--- train ---\")\n",
    "        ae_model.fit([X_node, As], [X_node[:,0]],\n",
    "                  epochs = ae_epochs_each,\n",
    "                  batch_size = ae_batch_size)\n",
    "        print(\"--- public ---\")\n",
    "        ae_model.fit([X_node_pub, As_pub], [X_node_pub[:,0]],\n",
    "                  epochs = ae_epochs_each,\n",
    "                  batch_size = ae_batch_size)\n",
    "        print(\"--- private ---\")\n",
    "        ae_model.fit([X_node_pri, As_pri], [X_node_pri[:,0]],\n",
    "                  epochs = ae_epochs_each,\n",
    "                  batch_size = ae_batch_size)\n",
    "        gc.collect()\n",
    "    print(\"****** save ae model ******\")\n",
    "    base.save_weights(\"./base_ae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "```\n",
    "start:15:38\n",
    "end:16:16\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ fold 0 start -----\n",
      "------ fold 0 start -----\n",
      "------ fold 0 start -----\n",
      "****** load ae model ******\n",
      "epochs : 30, batch_size : 8\n",
      "Epoch 1/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.5702\n",
      "Epoch 2/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.3342\n",
      "Epoch 3/30\n",
      "210/210 [==============================] - 17s 81ms/step - loss: 0.3048 - val_loss: 0.2831\n",
      "Epoch 4/30\n",
      "210/210 [==============================] - 10s 50ms/step - loss: 0.2877\n",
      "Epoch 5/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2795\n",
      "Epoch 6/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2720 - val_loss: 0.2604\n",
      "Epoch 7/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2658\n",
      "Epoch 8/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2619\n",
      "Epoch 9/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2568 - val_loss: 0.2508\n",
      "Epoch 10/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2535\n",
      "Epoch 11/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2496\n",
      "Epoch 12/30\n",
      "210/210 [==============================] - 11s 53ms/step - loss: 0.2444 - val_loss: 0.2473\n",
      "Epoch 13/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2433\n",
      "Epoch 14/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2394\n",
      "Epoch 15/30\n",
      "210/210 [==============================] - 11s 53ms/step - loss: 0.2358 - val_loss: 0.2409\n",
      "Epoch 16/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2339\n",
      "Epoch 17/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2330\n",
      "Epoch 18/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2309 - val_loss: 0.2371\n",
      "Epoch 19/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2286\n",
      "Epoch 20/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2257\n",
      "Epoch 21/30\n",
      "210/210 [==============================] - 11s 53ms/step - loss: 0.2255 - val_loss: 0.2389\n",
      "Epoch 22/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2242\n",
      "Epoch 23/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2211\n",
      "Epoch 24/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2193 - val_loss: 0.2355\n",
      "Epoch 25/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2164\n",
      "Epoch 26/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2163\n",
      "Epoch 27/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2146 - val_loss: 0.2357\n",
      "Epoch 28/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2128\n",
      "Epoch 29/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2116\n",
      "Epoch 30/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2115 - val_loss: 0.2346\n",
      "epochs : 10, batch_size : 16\n",
      "Epoch 1/10\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.2062\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.2024\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.2013 - val_loss: 0.2294\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.1999\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 5s 47ms/step - loss: 0.1992\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.1995 - val_loss: 0.2319\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 5s 47ms/step - loss: 0.1980\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.1977\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 6s 52ms/step - loss: 0.1974 - val_loss: 0.2308\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.1961\n",
      "epochs : 3, batch_size : 32\n",
      "Epoch 1/3\n",
      "53/53 [==============================] - 3s 50ms/step - loss: 0.1929\n",
      "Epoch 2/3\n",
      "53/53 [==============================] - 3s 48ms/step - loss: 0.1912\n",
      "Epoch 3/3\n",
      "53/53 [==============================] - 3s 57ms/step - loss: 0.1912 - val_loss: 0.2289\n",
      "epochs : 3, batch_size : 64\n",
      "Epoch 1/3\n",
      "27/27 [==============================] - 2s 63ms/step - loss: 0.1891\n",
      "Epoch 2/3\n",
      "27/27 [==============================] - 2s 63ms/step - loss: 0.1879\n",
      "Epoch 3/3\n",
      "27/27 [==============================] - 2s 83ms/step - loss: 0.1870 - val_loss: 0.2284\n",
      "epochs : 5, batch_size : 128\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 1s 94ms/step - loss: 0.1864\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 1s 97ms/step - loss: 0.1864\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 0.1860 - val_loss: 0.2286\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 1s 96ms/step - loss: 0.1860\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 1s 95ms/step - loss: 0.1860\n",
      "epochs : 5, batch_size : 256\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 2s 252ms/step - loss: 0.1854\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.1847\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 2s 223ms/step - loss: 0.1843 - val_loss: 0.2283\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.1843\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 0.1836\n",
      "fold 0: mcrmse 0.2282829106060687\n",
      "------ fold 1 start -----\n",
      "------ fold 1 start -----\n",
      "------ fold 1 start -----\n",
      "****** load ae model ******\n",
      "epochs : 30, batch_size : 8\n",
      "Epoch 1/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.5310\n",
      "Epoch 2/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.3322\n",
      "Epoch 3/30\n",
      "210/210 [==============================] - 16s 75ms/step - loss: 0.3046 - val_loss: 0.2865\n",
      "Epoch 4/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2887\n",
      "Epoch 5/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2796\n",
      "Epoch 6/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2725 - val_loss: 0.2526\n",
      "Epoch 7/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2663\n",
      "Epoch 8/30\n",
      "210/210 [==============================] - 10s 50ms/step - loss: 0.2617\n",
      "Epoch 9/30\n",
      "210/210 [==============================] - 11s 53ms/step - loss: 0.2570 - val_loss: 0.2442\n",
      "Epoch 10/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2535\n",
      "Epoch 11/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2504\n",
      "Epoch 12/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2468 - val_loss: 0.2429\n",
      "Epoch 13/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2447\n",
      "Epoch 14/30\n",
      "210/210 [==============================] - 10s 50ms/step - loss: 0.2406\n",
      "Epoch 15/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2380 - val_loss: 0.2398\n",
      "Epoch 16/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2348\n",
      "Epoch 17/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2326\n",
      "Epoch 18/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2316 - val_loss: 0.2347\n",
      "Epoch 19/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2282\n",
      "Epoch 20/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2277\n",
      "Epoch 21/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2242 - val_loss: 0.2308\n",
      "Epoch 22/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2244\n",
      "Epoch 23/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2223\n",
      "Epoch 24/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2205 - val_loss: 0.2306\n",
      "Epoch 25/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2188\n",
      "Epoch 26/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2186\n",
      "Epoch 27/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2153 - val_loss: 0.2298\n",
      "Epoch 28/30\n",
      "210/210 [==============================] - 10s 50ms/step - loss: 0.2136\n",
      "Epoch 29/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2130\n",
      "Epoch 30/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2112 - val_loss: 0.2331\n",
      "epochs : 10, batch_size : 16\n",
      "Epoch 1/10\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.2053\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.2025\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.2013 - val_loss: 0.2272\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.2005\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.2000\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.1989 - val_loss: 0.2240\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.1989\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.1980\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.1977 - val_loss: 0.2249\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 5s 47ms/step - loss: 0.1969\n",
      "epochs : 3, batch_size : 32\n",
      "Epoch 1/3\n",
      "53/53 [==============================] - 3s 50ms/step - loss: 0.1938\n",
      "Epoch 2/3\n",
      "53/53 [==============================] - 3s 50ms/step - loss: 0.1915\n",
      "Epoch 3/3\n",
      "53/53 [==============================] - 3s 57ms/step - loss: 0.1909 - val_loss: 0.2240\n",
      "epochs : 3, batch_size : 64\n",
      "Epoch 1/3\n",
      "27/27 [==============================] - 2s 64ms/step - loss: 0.1895\n",
      "Epoch 2/3\n",
      "27/27 [==============================] - 2s 63ms/step - loss: 0.1882\n",
      "Epoch 3/3\n",
      "27/27 [==============================] - 2s 82ms/step - loss: 0.1874 - val_loss: 0.2229\n",
      "epochs : 5, batch_size : 128\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 1s 96ms/step - loss: 0.1871\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 1s 97ms/step - loss: 0.1869\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 0.1861 - val_loss: 0.2224\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 1s 97ms/step - loss: 0.1856\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 1s 96ms/step - loss: 0.1858\n",
      "epochs : 5, batch_size : 256\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 2s 261ms/step - loss: 0.1859\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 0.1852\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 2s 231ms/step - loss: 0.1845 - val_loss: 0.2220\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 0.1845\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.1842\n",
      "fold 1: mcrmse 0.2217718478046174\n",
      "------ fold 2 start -----\n",
      "------ fold 2 start -----\n",
      "------ fold 2 start -----\n",
      "****** load ae model ******\n",
      "epochs : 30, batch_size : 8\n",
      "Epoch 1/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.5476\n",
      "Epoch 2/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.3276\n",
      "Epoch 3/30\n",
      "210/210 [==============================] - 16s 75ms/step - loss: 0.2994 - val_loss: 0.2917\n",
      "Epoch 4/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2861\n",
      "Epoch 5/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2777\n",
      "Epoch 6/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2689 - val_loss: 0.2681\n",
      "Epoch 7/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2633\n",
      "Epoch 8/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2596\n",
      "Epoch 9/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2550 - val_loss: 0.2574\n",
      "Epoch 10/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2504\n",
      "Epoch 11/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2462\n",
      "Epoch 12/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2446 - val_loss: 0.2652\n",
      "Epoch 13/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2416\n",
      "Epoch 14/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2376\n",
      "Epoch 15/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2351 - val_loss: 0.2557\n",
      "Epoch 16/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2335\n",
      "Epoch 17/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2303\n",
      "Epoch 18/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2279 - val_loss: 0.2455\n",
      "Epoch 19/30\n",
      "210/210 [==============================] - 10s 46ms/step - loss: 0.2258\n",
      "Epoch 20/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2251\n",
      "Epoch 21/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2232 - val_loss: 0.2407\n",
      "Epoch 22/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2215\n",
      "Epoch 23/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2178\n",
      "Epoch 24/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2165 - val_loss: 0.2422\n",
      "Epoch 25/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2157\n",
      "Epoch 26/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2137\n",
      "Epoch 27/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2130 - val_loss: 0.2406\n",
      "Epoch 28/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2113\n",
      "Epoch 29/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2104\n",
      "Epoch 30/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2080 - val_loss: 0.2375\n",
      "epochs : 10, batch_size : 16\n",
      "Epoch 1/10\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.2030\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.2001\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.1990 - val_loss: 0.2370\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 5s 46ms/step - loss: 0.1983\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.1978\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.1971 - val_loss: 0.2353\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.1967\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 5s 47ms/step - loss: 0.1955\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.1955 - val_loss: 0.2349\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 5s 47ms/step - loss: 0.1943\n",
      "epochs : 3, batch_size : 32\n",
      "Epoch 1/3\n",
      "53/53 [==============================] - 3s 50ms/step - loss: 0.1915\n",
      "Epoch 2/3\n",
      "53/53 [==============================] - 3s 51ms/step - loss: 0.1895\n",
      "Epoch 3/3\n",
      "53/53 [==============================] - 3s 57ms/step - loss: 0.1880 - val_loss: 0.2354\n",
      "epochs : 3, batch_size : 64\n",
      "Epoch 1/3\n",
      "27/27 [==============================] - 2s 63ms/step - loss: 0.1875\n",
      "Epoch 2/3\n",
      "27/27 [==============================] - 2s 64ms/step - loss: 0.1859\n",
      "Epoch 3/3\n",
      "27/27 [==============================] - 2s 73ms/step - loss: 0.1852 - val_loss: 0.2342\n",
      "epochs : 5, batch_size : 128\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 1s 95ms/step - loss: 0.1847\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 1s 96ms/step - loss: 0.1839\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 0.1840 - val_loss: 0.2347\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 1s 95ms/step - loss: 0.1837\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 1s 95ms/step - loss: 0.1836\n",
      "epochs : 5, batch_size : 256\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.1835\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.1834\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 1s 191ms/step - loss: 0.1827 - val_loss: 0.2339\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.1827\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.1819\n",
      "fold 2: mcrmse 0.2345790221072678\n",
      "------ fold 3 start -----\n",
      "------ fold 3 start -----\n",
      "------ fold 3 start -----\n",
      "****** load ae model ******\n",
      "epochs : 30, batch_size : 8\n",
      "Epoch 1/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.5734\n",
      "Epoch 2/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.3378\n",
      "Epoch 3/30\n",
      "210/210 [==============================] - 16s 75ms/step - loss: 0.3040 - val_loss: 0.2814\n",
      "Epoch 4/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2894\n",
      "Epoch 5/30\n",
      "210/210 [==============================] - 10s 46ms/step - loss: 0.2785\n",
      "Epoch 6/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2715 - val_loss: 0.2783\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2659\n",
      "Epoch 8/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2600\n",
      "Epoch 9/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2540 - val_loss: 0.2571\n",
      "Epoch 10/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2513\n",
      "Epoch 11/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2475\n",
      "Epoch 12/30\n",
      "210/210 [==============================] - 11s 55ms/step - loss: 0.2450 - val_loss: 0.2534\n",
      "Epoch 13/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2427\n",
      "Epoch 14/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2389\n",
      "Epoch 15/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2368 - val_loss: 0.2547\n",
      "Epoch 16/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2352\n",
      "Epoch 17/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2317\n",
      "Epoch 18/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2289 - val_loss: 0.2504\n",
      "Epoch 19/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2281\n",
      "Epoch 20/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2260\n",
      "Epoch 21/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2229 - val_loss: 0.2458\n",
      "Epoch 22/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2218\n",
      "Epoch 23/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2201\n",
      "Epoch 24/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2181 - val_loss: 0.2447\n",
      "Epoch 25/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2171\n",
      "Epoch 26/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2170\n",
      "Epoch 27/30\n",
      "210/210 [==============================] - 11s 53ms/step - loss: 0.2138 - val_loss: 0.2426\n",
      "Epoch 28/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2123\n",
      "Epoch 29/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2110\n",
      "Epoch 30/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2106 - val_loss: 0.2420\n",
      "epochs : 10, batch_size : 16\n",
      "Epoch 1/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.2046\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.2019\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 6s 54ms/step - loss: 0.2004 - val_loss: 0.2404\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.1999\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 5s 49ms/step - loss: 0.1995\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 6s 53ms/step - loss: 0.1980 - val_loss: 0.2384\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.1978\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.1969\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 6s 55ms/step - loss: 0.1951 - val_loss: 0.2390\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.1963\n",
      "epochs : 3, batch_size : 32\n",
      "Epoch 1/3\n",
      "53/53 [==============================] - 3s 51ms/step - loss: 0.1935\n",
      "Epoch 2/3\n",
      "53/53 [==============================] - 3s 51ms/step - loss: 0.1915\n",
      "Epoch 3/3\n",
      "53/53 [==============================] - 3s 59ms/step - loss: 0.1900 - val_loss: 0.2380\n",
      "epochs : 3, batch_size : 64\n",
      "Epoch 1/3\n",
      "27/27 [==============================] - 2s 63ms/step - loss: 0.1889\n",
      "Epoch 2/3\n",
      "27/27 [==============================] - 2s 64ms/step - loss: 0.1870\n",
      "Epoch 3/3\n",
      "27/27 [==============================] - 2s 75ms/step - loss: 0.1871 - val_loss: 0.2379\n",
      "epochs : 5, batch_size : 128\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 1s 96ms/step - loss: 0.1865\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 1s 96ms/step - loss: 0.1864\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 2s 116ms/step - loss: 0.1858 - val_loss: 0.2394\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 1s 97ms/step - loss: 0.1852\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 1s 95ms/step - loss: 0.1852\n",
      "epochs : 5, batch_size : 256\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.1845\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 1s 149ms/step - loss: 0.1850\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 1s 185ms/step - loss: 0.1840 - val_loss: 0.2383\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 1s 150ms/step - loss: 0.1839\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 1s 147ms/step - loss: 0.1838\n",
      "fold 3: mcrmse 0.23877804618956153\n",
      "------ fold 4 start -----\n",
      "------ fold 4 start -----\n",
      "------ fold 4 start -----\n",
      "****** load ae model ******\n",
      "epochs : 30, batch_size : 8\n",
      "Epoch 1/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.5485\n",
      "Epoch 2/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.3343\n",
      "Epoch 3/30\n",
      "210/210 [==============================] - 16s 76ms/step - loss: 0.3053 - val_loss: 0.2755\n",
      "Epoch 4/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2883\n",
      "Epoch 5/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2790\n",
      "Epoch 6/30\n",
      "210/210 [==============================] - 11s 53ms/step - loss: 0.2719 - val_loss: 0.2620\n",
      "Epoch 7/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2653\n",
      "Epoch 8/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2623\n",
      "Epoch 9/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2549 - val_loss: 0.2489\n",
      "Epoch 10/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2518\n",
      "Epoch 11/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2480\n",
      "Epoch 12/30\n",
      "210/210 [==============================] - 11s 54ms/step - loss: 0.2449 - val_loss: 0.2467\n",
      "Epoch 13/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2418\n",
      "Epoch 14/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2397\n",
      "Epoch 15/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2365 - val_loss: 0.2413\n",
      "Epoch 16/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2341\n",
      "Epoch 17/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2321\n",
      "Epoch 18/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2305 - val_loss: 0.2482\n",
      "Epoch 19/30\n",
      "210/210 [==============================] - 10s 46ms/step - loss: 0.2275\n",
      "Epoch 20/30\n",
      "210/210 [==============================] - 10s 47ms/step - loss: 0.2257\n",
      "Epoch 21/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2234 - val_loss: 0.2367\n",
      "Epoch 22/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2225\n",
      "Epoch 23/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2207\n",
      "Epoch 24/30\n",
      "210/210 [==============================] - 11s 51ms/step - loss: 0.2184 - val_loss: 0.2363\n",
      "Epoch 25/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2170\n",
      "Epoch 26/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2161\n",
      "Epoch 27/30\n",
      "210/210 [==============================] - 11s 54ms/step - loss: 0.2132 - val_loss: 0.2328\n",
      "Epoch 28/30\n",
      "210/210 [==============================] - 10s 49ms/step - loss: 0.2134\n",
      "Epoch 29/30\n",
      "210/210 [==============================] - 10s 48ms/step - loss: 0.2119\n",
      "Epoch 30/30\n",
      "210/210 [==============================] - 11s 52ms/step - loss: 0.2103 - val_loss: 0.2349\n",
      "epochs : 10, batch_size : 16\n",
      "Epoch 1/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.2040\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.2019\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 5s 52ms/step - loss: 0.2006 - val_loss: 0.2316\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.1999\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 5s 48ms/step - loss: 0.1986\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 6s 52ms/step - loss: 0.1982 - val_loss: 0.2316\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 5s 47ms/step - loss: 0.1978\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 5s 47ms/step - loss: 0.1967\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 5s 51ms/step - loss: 0.1960 - val_loss: 0.2307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "105/105 [==============================] - 5s 47ms/step - loss: 0.1956\n",
      "epochs : 3, batch_size : 32\n",
      "Epoch 1/3\n",
      "53/53 [==============================] - 3s 49ms/step - loss: 0.1925\n",
      "Epoch 2/3\n",
      "53/53 [==============================] - 3s 49ms/step - loss: 0.1918\n",
      "Epoch 3/3\n",
      "53/53 [==============================] - 3s 57ms/step - loss: 0.1910 - val_loss: 0.2315\n",
      "epochs : 3, batch_size : 64\n",
      "Epoch 1/3\n",
      "27/27 [==============================] - 2s 62ms/step - loss: 0.1885\n",
      "Epoch 2/3\n",
      "27/27 [==============================] - 2s 62ms/step - loss: 0.1874\n",
      "Epoch 3/3\n",
      "27/27 [==============================] - 2s 73ms/step - loss: 0.1871 - val_loss: 0.2305\n",
      "epochs : 5, batch_size : 128\n",
      "Epoch 1/5\n",
      "14/14 [==============================] - 1s 95ms/step - loss: 0.1866\n",
      "Epoch 2/5\n",
      "14/14 [==============================] - 1s 97ms/step - loss: 0.1858\n",
      "Epoch 3/5\n",
      "14/14 [==============================] - 2s 115ms/step - loss: 0.1857 - val_loss: 0.2313\n",
      "Epoch 4/5\n",
      "14/14 [==============================] - 1s 96ms/step - loss: 0.1853\n",
      "Epoch 5/5\n",
      "14/14 [==============================] - 1s 96ms/step - loss: 0.1848\n",
      "epochs : 5, batch_size : 256\n",
      "Epoch 1/5\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.1849\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.1846\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 1s 190ms/step - loss: 0.1843 - val_loss: 0.2312\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 1s 148ms/step - loss: 0.1836\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 1s 146ms/step - loss: 0.1836\n",
      "fold 4: mcrmse 0.2319042850505944\n"
     ]
    }
   ],
   "source": [
    "## here train regression model from pretrain auto encoder model\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(5, shuffle = True, random_state = 42)\n",
    "\n",
    "scores = []\n",
    "preds = np.zeros([len(X_node), X_node.shape[1], 5])\n",
    "for i, (tr_idx, va_idx) in enumerate(kfold.split(X_node, As)):\n",
    "    print(f\"------ fold {i} start -----\")\n",
    "    print(f\"------ fold {i} start -----\")\n",
    "    print(f\"------ fold {i} start -----\")\n",
    "    X_node_tr = X_node[tr_idx]\n",
    "    X_node_va = X_node[va_idx]\n",
    "    As_tr = As[tr_idx]\n",
    "    As_va = As[va_idx]\n",
    "    y_tr = y[tr_idx]\n",
    "    y_va = y[va_idx]\n",
    "    \n",
    "    base = get_base(config)\n",
    "    if ae_epochs > 0:\n",
    "        print(\"****** load ae model ******\")\n",
    "        # 学習したモデルのパラメータを読み込んでる\n",
    "        base.load_weights(\"./base_ae\")\n",
    "    model = get_model(base, config)\n",
    "    if pretrain_dir is not None:\n",
    "        d = f\"./model{i}\"\n",
    "        print(f\"--- load from {d} ---\")\n",
    "        model.load_weights(d)\n",
    "    for epochs, batch_size in zip(epochs_list, batch_size_list):\n",
    "        print(f\"epochs : {epochs}, batch_size : {batch_size}\")\n",
    "        model.fit([X_node_tr, As_tr], [y_tr],\n",
    "                  validation_data=([X_node_va, As_va], [y_va]),\n",
    "                  epochs = epochs,\n",
    "                  batch_size = batch_size, validation_freq = 3)\n",
    "    # 再学習？    \n",
    "    model.save_weights(f\"./model{i}\")\n",
    "    p = model.predict([X_node_va, As_va])\n",
    "    scores.append(mcrmse(y_va, p))\n",
    "    print(f\"fold {i}: mcrmse {scores[-1]}\")\n",
    "    preds[va_idx] = p\n",
    "    if one_fold:\n",
    "        break\n",
    "        \n",
    "pd.to_pickle(preds, \"oof.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2282829106060687, 0.2217718478046174, 0.2345790221072678, 0.23877804618956153, 0.2319042850505944]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "```\n",
    "1 minuts\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "p_pub = 0\n",
    "p_pri = 0\n",
    "for i in range(5):\n",
    "    model.load_weights(f\"./model{i}\")\n",
    "    p_pub += model.predict([X_node_pub, As_pub]) / 5\n",
    "    p_pri += model.predict([X_node_pri, As_pri]) / 5\n",
    "    if one_fold:\n",
    "        p_pub *= 5\n",
    "        p_pri *= 5\n",
    "        break\n",
    "\n",
    "for i, target in enumerate(targets):\n",
    "    test_pub[target] = [list(p_pub[k, :, i]) for k in range(p_pub.shape[0])]\n",
    "    test_pri[target] = [list(p_pri[k, :, i]) for k in range(p_pri.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reactivity</th>\n",
       "      <th>deg_Mg_pH10</th>\n",
       "      <th>deg_pH10</th>\n",
       "      <th>deg_Mg_50C</th>\n",
       "      <th>deg_50C</th>\n",
       "      <th>id_seqpos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.825715</td>\n",
       "      <td>0.644734</td>\n",
       "      <td>1.818478</td>\n",
       "      <td>0.555845</td>\n",
       "      <td>0.747952</td>\n",
       "      <td>id_00073f8be_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.568830</td>\n",
       "      <td>3.392002</td>\n",
       "      <td>4.742649</td>\n",
       "      <td>3.301196</td>\n",
       "      <td>2.834531</td>\n",
       "      <td>id_00073f8be_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.774888</td>\n",
       "      <td>0.670446</td>\n",
       "      <td>0.879219</td>\n",
       "      <td>0.858390</td>\n",
       "      <td>0.826853</td>\n",
       "      <td>id_00073f8be_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.423639</td>\n",
       "      <td>1.085340</td>\n",
       "      <td>1.254038</td>\n",
       "      <td>1.613884</td>\n",
       "      <td>1.645869</td>\n",
       "      <td>id_00073f8be_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.846809</td>\n",
       "      <td>0.534353</td>\n",
       "      <td>0.594987</td>\n",
       "      <td>0.791237</td>\n",
       "      <td>0.829008</td>\n",
       "      <td>id_00073f8be_4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reactivity  deg_Mg_pH10  deg_pH10  deg_Mg_50C   deg_50C       id_seqpos\n",
       "0    0.825715     0.644734  1.818478    0.555845  0.747952  id_00073f8be_0\n",
       "1    2.568830     3.392002  4.742649    3.301196  2.834531  id_00073f8be_1\n",
       "2    1.774888     0.670446  0.879219    0.858390  0.826853  id_00073f8be_2\n",
       "3    1.423639     1.085340  1.254038    1.613884  1.645869  id_00073f8be_3\n",
       "4    0.846809     0.534353  0.594987    0.791237  0.829008  id_00073f8be_4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_ls = []\n",
    "for df, preds in [(test_pub, p_pub), (test_pri, p_pri)]:\n",
    "    for i, uid in enumerate(df.id):\n",
    "        single_pred = preds[i]\n",
    "\n",
    "        single_df = pd.DataFrame(single_pred, columns=targets)\n",
    "        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n",
    "\n",
    "        preds_ls.append(single_df)\n",
    "\n",
    "preds_df = pd.concat(preds_ls)\n",
    "preds_df.to_csv(\"../submission/submission_20201004.csv\", index = False)\n",
    "preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLB 0.24895"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
